dataAnalysis_Long_3_23 <- readRDS("C:/Users/searsja1/Dropbox/rural/ProcessedData/dataAnalysis_Long_3_23.rds")
colnames(dataAnalysis_Long_3_23)
pol_df <- select(dataAnalysis_Long_3_23,
state:county_name,
stayhomes:emergency)
pacman::p_load(tidyverse)
pol_df <- select(dataAnalysis_Long_3_23,
state:county_name,
stayhomes:emergency)
View(pol_df)
pol_df <- select(dataAnalysis_Long_3_23,
state:county_name,
stayhomes:emergency) %>%
distinct(county_fips, .keep_all = T)
View(pol_df)
pol_df <- select(dataAnalysis_Long_3_23,
state:county_name,
event_date_stayhomex) %>%
distinct(county_fips, .keep_all = T)
pol_df <- select(dataAnalysis_Long_3_23,
state:county_name,
event_date_stayhomec) %>%
distinct(county_fips, .keep_all = T)
View(pol_df)
pacman::p_load(readr, tidyverse)
getwd()
write_csv("C:/Users/searsja1/OneDrive - Michigan State University/Research/Rural Mobility/Processed Data/county_lockdowns.csv")
write_csv(pol_df, "C:/Users/searsja1/OneDrive - Michigan State University/Research/Rural Mobility/Processed Data/county_lockdowns.csv")
options(htmltools.dir.version = FALSE)
library(knitr)
library(fontawesome)
knitr::opts_chunk$set(
fig.align = "center",
cache = FALSE,
dpi = 300,
warning = F,
message = F,
fig.height = 5,
out.width = "80%"
)
pacman::p_load(lubridate, rvest, tiydverse)
list <- html_elments(".puis-padding-right-micro")
list <- html_elements(".puis-padding-right-micro")
list <- amzn %>% html_elements(".puis-padding-right-micro")
amzn <- read_html("https://www.amazon.com/s?k=nike+alphafly&crid=3J5WHSJGNMIYA&sprefix=nike+alphafly%2Caps%2C117&ref=nb_sb_noss_1")
page1 <- read_html("https://www.yellowpages.com/search?search_terms=running+store&geo_location_terms=East+Lansing%2C+MI") %>%
get_elements("div.info")
page1 <- read_html("https://www.yellowpages.com/search?search_terms=running+store&geo_location_terms=East+Lansing%2C+MI") %>%
html_elements("div.info")
View(page1)
page1[[1]]
rm(page1)
yp <- read_html("https://www.yellowpages.com/search?search_terms=running+store&geo_location_terms=East+Lansing%2C+MI") %>%
html_elements("div.info")
page1 <- read_html("https://www.yellowpages.com/search?search_terms=running+store&geo_location_terms=East+Lansing%2C+MI") %>%
html_elements("div.info")
page1[[1]]
```
page1[[1]]
```
page1 <- read_html("https://www.yellowpages.com/search?search_terms=running+store&geo_location_terms=East+Lansing%2C+MI") %>%
html_elements("div.info")
page1 <- read_html("https://www.yellowpages.com/search?search_terms=running+store&geo_location_terms=East+Lansing%2C+MI") %>%
html_elements("div.info")
page1 <- r
page1[[1]]
inf_prim <- page1[[1]] %>% # start with first listing box
html_element("div.info-section info-primary")
inf_prim <- page1[[1]] %>% # start with first listing box
html_element("div.info-section.info-primary")
inf_prim <- page1[[1]] %>% # start with first listing box
html_element("div.info-section.info-primary") %>% # grab primary info
html_table()
inf_prim <- page1[[1]] %>% # start with first listing box
html_element("div.info-section.info-primary")
inf_prim
inf_prim
```
name <- page1[[1]] %>% # start with first listing box
html_element("div.info-section.info-primary") %>%
html_element("h2.business-name") %>%
html_text()
name
name <- page1[[1]] %>% # start with first listing box
html_element("div.info-section.info-primary") %>%
html_element("h2.business-name") %>%
name
name <- page1[[1]] %>% # start with first listing box
html_element("div.info-section.info-primary") %>%
html_element("h2.business-name")
name
name <- page1[[1]] %>% # start with first listing box
html_element("div.info-section.info-primary") %>%
html_element("a.business-name")
name
inf_prim <- page1[[1]] %>% # start with first listing box
html_element("div.info-section.info-primary")
inf_prim
name <- page1[[1]] %>% # start with first listing box
html_element("div.info-section.info-primary") %>%
html_element("a.business-name")
name
options(htmltools.dir.version = FALSE)
library(knitr)
library(fontawesome)
knitr::opts_chunk$set(
fig.align = "center",
cache = FALSE,
dpi = 300,
warning = F,
message = F,
fig.height = 5,
out.width = "80%"
)
pacman::p_load(lubridate, rvest, tiydverse)
```
---
class: inverse, middle
name: static
# Scrapign Static Websites
---
# Scraping Static Sites
Last time we saw how to use .hi-slate[rvest] functions to scrape information from .hi-medgrn[HTML Tables].
* .hi-blue[Biggest Challenge:] finding the right CSS selectors
--
Sometimes, however, the data we want to scrape *aren't* in a nice table format already.
Let's work through an example: Yellow Pages.
---
# Application 2: Yellow Pages
Let's say you got excited by all this marathon talk and want to start running.
--
.less-left[
Naturally, you go to [Yellowpages.com](https://www.yellowpages.com/search?search_terms=running+store&geo_location_terms=East+Lansing%2C+MI) and search for "running stores" in East Lansing]
.more-right[
<img src = "images/yp_1.png" /img>
]
---
# SelectorGadget vs. Source
Let's scrape the descriptive info for each of the stores to help us figure out where to go first.
Try using SelectorGadget to find the selector for the info in [Playmakers](https://playmakers.com/)' box.
.center[
<img src = "images/yp_2.png" width = "500" /img>
]
---
# SelectorGadget vs. Source
Were you able to get it? I wasn't. `r fa('face-frown')`
--
This is an example of a case where we need to .hi-blue[inspect the source] to find the selector. Give it a try now!
---
# SelectorGadget vs. Source
After a little digging you'll find the selector to be `"div.info"`
.center[
<img src = "images/yp_2.png" /img>
]
---
# SelectorGadget vs. Source
.hi-medgrn[One hiccup:] this selector isn't unique!
.center[
<img src = "images/yp_dup.png"  /img>
]
---
# Duplicate Elements with html_elements()
.hi-blue[Solution:] use `html_elements()` to retrieve all the box contents at once.
page1 <- read_html("https://www.yellowpages.com/search?search_terms=running+store&geo_location_terms=East+Lansing%2C+MI") %>%
html_elements("div.info")
```
---
# Application 2: Yellow Pages
We got back a .hi-medgrn[list] with 7 elements, one for each of the stores listed on the first page.
What does the Playmakers element (first list object) contain?
page1[[1]]
inf_prim <- page1[[1]] %>% # start with first listing box
html_element("div.info-section.info-primary") %>% # grab primary info
html_table()
inf_prim
inf_prim <- page1[[1]] %>% # start with first listing box
html_element("div.info-section.info-primary")
inf_prim
name <- page1[[1]] %>% # start with first listing box
html_element("div.info-section.info-primary") %>%
html_element("a.business-name")
name
name <- page1[[1]] %>% # start with first listing box
html_element("div.info-section.info-primary") %>%
html_element("a.business-name") %>%
html_text()
name
name <- page1[[1]] %>% # start with first listing box
html_element("div.info-section.info-primary") %>% # get primary info division
html_element("a.business-name")
name
?html_attr
?html_text
name %>% html_text() # get text portion of element
name %>% html_text2()
page1 <- read_html("https://www.yellowpages.com/search?search_terms=running+store&geo_location_terms=East+Lansing%2C+MI") %>%
html_elements("div.info")
page1 <- read_html("https://www.yellowpages.com/search?search_terms=running+store&geo_location_terms=East+Lansing%2C+MI") %>%
html_elements("div.info")
page1
page1[[1]]
inf_prim <- page1[[1]] %>% # start with first listing box
html_element("div.info-section.info-primary") %>% # grab primary info
html_table()
inf_prim
inf_prim <- page1[[1]] %>% # start with first listing box
html_element("div.info-section.info-primary")
inf_prim
name <- page1[[1]] %>% # start with first listing box
html_element("div.info-section.info-primary") %>% # get primary info division
html_element("a.business-name")
name
---
# Application 2: Yellow Pages
```{r}
cat <- inf_prim %>%
html_element("div.categories") %>%
html_table()
cat <- inf_prim %>%
html_element("div.categories") %>%
html_text()
cat
cat <- inf_prim %>%
html_element("div.categories") %>%
html_text()
cat
cat <- inf_prim %>%
html_element("div.categories") %>%
html_text2()
cat
cat <- inf_prim %>%
html_element("div.categories")
cat
cat <- inf_prim %>%
html_element("div.categories") %>%
html_text2("a")
cat <- inf_prim %>%
html_element("div.categories") %>%
html_text2()
cat
options(htmltools.dir.version = FALSE)
library(knitr)
library(fontawesome)
knitr::opts_chunk$set(
fig.align = "center",
cache = FALSE,
dpi = 300,
warning = F,
message = F,
fig.height = 5,
out.width = "80%"
)
pacman::p_load(lubridate, rvest, tiydverse)
```
---
class: inverse, middle
name: static
# Scraping Static Websites
---
# Scraping Static Sites
Last time we saw how to use .hi-slate[rvest] functions to scrape information from .hi-medgrn[HTML Tables].
* .hi-blue[Biggest Challenge:] finding the right CSS selectors
--
<br>
Sometimes, however, the data we want to scrape *aren't* in a nice table format already.
Let's work through an example: .hi-yellow[Yellow Pages].
---
# Application 2: Yellow Pages
Let's say you got excited by all this marathon talk and want to start running.
--
.less-left[
Naturally, you go to [Yellowpages.com](https://www.yellowpages.com/east-lansing-mi/running-stores) and search for "running stores" in East Lansing]
.more-right[
<img src = "images/yp_1.png" /img>
]
---
# SelectorGadget vs. Source
Let's scrape the descriptive info for each of the stores to help us figure out where to go first.
Try using SelectorGadget to find the selector for the info in [Playmakers](https://playmakers.com/)' box.
.center[
<img src = "images/yp_2.png" width = "500" /img>
]
---
# SelectorGadget vs. Source
Were you able to get it? I wasn't. `r fa('face-frown')`
--
This is an example of a case where we need to .hi-blue[inspect the source] to find the selector. Give it a try now!
---
# SelectorGadget vs. Source
After a little digging you'll find the selector to be `"div.info"`
.center[
<img src = "images/yp_source.png" /img>
]
---
# SelectorGadget vs. Source
.hi-medgrn[One hiccup:] this selector isn't unique!
.center[
<img src = "images/yp_dup.png"  /img>
]
---
# Duplicate Elements with html_elements()
.hi-blue[Solution:] use `html_elements()` to retrieve .hi-blue[all matching elements at once]
page1 <- read_html("https://www.yellowpages.com/east-lansing-mi/running-stores") %>%
html_elements("div.info")
page1
```
---
# Application 2: Yellow Pages
We got back a .hi-medgrn[list] with 7 elements, one for each of the stores listed on the first page of search results.
What does the .hi-pink[Playmakers element] (first list object) contain?
page1[[1]]
inf_prim <- page1[[1]] %>% # start with first listing box
html_element("div.info-section.info-primary") %>% # grab primary info
html_table()
inf_prim
inf_prim <- page1[[1]] %>% # start with first listing box
html_element("div.info-section.info-primary")
inf_prim
name <- page1[[1]] %>% # start with first listing box
html_element("div.info-section.info-primary") %>% # get primary info division
html_element("a.business-name")
name
name %>% html_text() # get raw text portion of element
name %>% html_text2() # get text as it appears online
```
---
# Retrieve Link with *html_attr()*
.hi-medgrn[2\. Retrieve the Link (Business Page):]
* `html_attr()` gets an attribute with a particular name (here the hyperlink, or "href")
name %>%
html_attr("href") # get link portion of element (link after "href =")
cat <- inf_prim %>%
html_element("div.categories") %>%
html_table()
cat
cat <- inf_prim %>%
html_element("div.categories") %>%
html_text2()
cat
cat <- inf_prim %>%
html_element("div.categories") %>%
html_text()
cat
?html_inf_prim %>%
html_element("div.categories")
inf_prim %>%
html_element("div.categories")
inf_prim %>%
html_element("div.categories")[1]
inf_prim %>%
html_element("div.categories")[[1]]
inf_prim %>%
html_element("div.categories")
cat <- inf_prim %>%
html_element("div.categories") %>%
html_element("nth-child(1)")
?html_child
?html_children
cat <- inf_prim %>%
html_element("div.categories") %>%
html_children()
cat
cat[1]
cat[1] %>% html_text()
?html_text
cat <- inf_prim %>%
html_element("div.categories") %>%
html_text(perserve_nbsp = T)
cat <- inf_prim %>%
html_element("div.categories") %>%
html_text2(perserve_nbsp = T)
cat <- inf_prim %>%
html_element("div.categories") %>%
html_text2(preserve_nbsp = T)
cat
cat <- inf_prim %>%
html_element("div.categories") %>%
html_text2() %>% writeLines*()
cat <- inf_prim %>%
html_element("div.categories") %>%
html_text2() %>% writeLines()
cat
cat <- inf_prim %>%
html_element("div.categories") %>%
html_text2()
cat
inf_prim %>%
html_element("div.categories") %>%
html_children()
pm <- read_html(paste0("www.yellowpages.com", name %>% html_attr("href") ))
pm <- read_html(paste0("yellowpages.com", name %>% html_attr("href") ))
pm <- read_html("https://www.yellowpages.com/okemos-mi/mip/playmakers-5974526")
link <- paste0("https://yellowpages.com/", name %>% html_attr("href"))
pm <- read_html(link)
pm
link <- paste0("https://yellowpages.com/", name %>% html_attr("href"))
pm <- read_html(link)
pm <- read_html(link)
cat <- pm %>% html_element("div.categories") %>%
html_text()
cat
cat <- pm %>% html_element("div.categories") %>%
html_text2()
cat
cat <- pm %>% html_element("dd.weblinks") %>%
html_text2()
cat
cat <- pm %>% html_element("dd.weblinks") %>%
html_text()
cat
cat <- pm %>% html_element("div.categories") %>%
html_text() %>%
str_split(", ")
cat
pm %>% html_element("div.categories") %>%
html_table()
cat <- inf_prim %>%
html_element("div.categories") %>%
html_text()
inf_prim %>%
html_element("div.categories") %>%
html_text()
pm %>%
html_element("div.categories") %>%
html_text()
pm %>%
html_element("div.categories") %>%
html_children()
pm %>%
html_element("div.categories") %>%
html_text()
pm %>%
html_element("div.categories") %>%
html_text2()
pm %>%
html_element("div.categories") %>%
html_children()
pm %>% html_element("dd.other-information") %>%
html_text()
pm %>% html_element("dd.other-information") %>%
html_text()
hours <- pm %>% html_element("dd.open-hours")
hours %>% html_table()
hours %>% html_table()
hours_df <- hours %>% html_table()
View(hours_df)
colnames(hours_df) <- c("Days", "Business Hours")
View(hours_df)
hours_df <- mutate(hours_df,
Days = str_replace(Days, ":$", ""))
View(hours_df)
yp <- read_html("https://www.yellowpages.com/")
html_form(yp)
yp <- read_html("https://www.yellowpages.com/")
yp <- read_html("https://www.yellowpages.com/")
# grab the search form
search <- html_form(yp)[[1]]
search
session(yp)
session(yp)
session("https://www.yellowpages.com/")
search_set <- html_form_set(search,
search_terms = "Running Store",
geo_location_terms = "Grand Rapids, MI")
resp <- html_form_submit(search_set)
resp
